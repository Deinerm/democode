---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r}
library(RefManageR)
bib <- ReadBib('./bib/test.bib')
title <- unlist(bib$title)
year <- unlist(bib$urldate)
abstract <- unlist(bib$abstract)
journal <- unlist(bib$journal)

library(lubridate)
library(tidyverse)
library(tidytext)
library(stringr)

time = as.POSIXct(year, origin = "1970-01-01")
month = round_date(time, "month")

tbl <- tbl_df(cbind(title = title[1:185],time, month ,abstract = abstract[1:185],journal = journal[1:185]))

titlewords <- unnest_tokens(tbl, word,abstract) %>%
        anti_join(stop_words, by = "word") %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()

word_counts <- titlewords %>%
  count(word, sort = TRUE)

word_counts %>%
  head(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Most common words in Hacker News titles",
       subtitle = "Among the last million stories; stop words removed",
       y = "# of uses")

stories_per_month <- tbl %>%
  group_by(month) %>%
  summarize(month_total = n())

word_month_counts <- titlewords %>%
  filter(word_total >= 50)%>%
  count(word, month) %>%
  complete(word, month, fill = list(n = 0)) %>%
  inner_join(stories_per_month, by = "month") %>%
  mutate(percent = n / month_total)


```

```{r pubmed.mineR}
library(pubmed.mineR)
library(easyPubMed)
library(tidyverse)
library(tidytext)
library(stringr)


my_query <- "Janusz Pawliszyn[AU]"
my_entrez_id <- get_pubmed_ids(my_query)
my_abstracts_xml <- fetch_pubmed_data(my_entrez_id)

# apply "saveXML" to each //ArticleTitle tag via XML::xpathApply()
my_titles <- unlist(xpathApply(my_abstracts_xml, "//ArticleTitle", saveXML))
#
# use gsub to remove the tag, also trim long titles
my_titles <- gsub("(^.{5,10}Title>)|(<\\/.*$)", "", my_titles)
my_titles[nchar(my_titles)>75] <- paste(substr(my_titles[nchar(my_titles)>75], 1, 70), 
                                        "...", sep = "")
print(my_titles)


out.A <- batch_pubmed_download(pubmed_query_string = my_query, 
                               format = "xml", 
                               batch_size = 300,
                               dest_file_prefix = "easyPM_example")

abs <- pubmed.mineR::xmlreadabs("./snp2016.xml")

abstext <- abs@Abstract
text_df <- data_frame(line = 1:length(abstext), text = abstext)
data("stop_words")
wordf <- text_df %>%
                filter(text!="No Abstract Found") %>%
                unnest_tokens(word, text) %>%
                anti_join(stop_words) %>%
                filter(str_detect(word, "[^\\d]")) 
#library(devtools)
#install_github("dgrtwo/widyr")

library(widyr)

title_word_pairs <- wordf %>% 
        pairwise_count(word, line, sort = TRUE, upper = FALSE)

library(ggplot2)
library(igraph)
library(ggraph)

set.seed(42)
title_word_pairs %>%
  filter(n >= 200) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

library(ggplot2)

desc_tf_idf <- wordf %>% 
  count(line, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, line, n)

desc_tf_idf %>% 
  arrange(-tf_idf)

library(topicmodels)

my_stop_words <- bind_rows(stop_words, 
                           data_frame(word = c('i','ii','iii',as.character(1:12)), 
                                      lexicon = rep("custom", 15)))

word_counts <- wordf %>%
  anti_join(my_stop_words) %>%
  count(line, word, sort = TRUE) %>%
  ungroup()

desc_dtm <- word_counts %>%
  cast_dtm(line, word, n)

desc_lda <- LDA(desc_dtm, k = 20, control = list(seed = 42))
# desc_lda1 <- desc_lda

tidy_lda <- tidy(desc_lda)
tidy_lda

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 5, scales = "free")

lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma

ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 5) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))

newsgroup_sentiments <- wordf %>%
  inner_join(get_sentiments("afinn"), by = "word") 
  summarize(score = sum(score * n) / sum(n))

contributions <- wordf %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions %>%
  top_n(25, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip()
```

