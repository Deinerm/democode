---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r}
library(RefManageR)
bib <- ReadBib('./bib/test.bib')
title <- unlist(bib$title)
year <- unlist(bib$urldate)
abstract <- unlist(bib$abstract)
journal <- unlist(bib$journal)

library(lubridate)
library(tidyverse)
library(tidytext)
library(stringr)

time = as.POSIXct(year, origin = "1970-01-01")
month = round_date(time, "month")

tbl <- tbl_df(cbind(title = title[1:185],time, month ,abstract = abstract[1:185],journal = journal[1:185]))

titlewords <- unnest_tokens(tbl, word,abstract) %>%
        anti_join(stop_words, by = "word") %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()

word_counts <- titlewords %>%
  count(word, sort = TRUE)

word_counts %>%
  head(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(title = "Most common words in Hacker News titles",
       subtitle = "Among the last million stories; stop words removed",
       y = "# of uses")

stories_per_month <- tbl %>%
  group_by(month) %>%
  summarize(month_total = n())

word_month_counts <- titlewords %>%
  filter(word_total >= 50)%>%
  count(word, month) %>%
  complete(word, month, fill = list(n = 0)) %>%
  inner_join(stories_per_month, by = "month") %>%
  mutate(percent = n / month_total)


```

```{r pubmed.mineR}
#library(devtools)
#install_github("dgrtwo/widyr")
library(easyPubMed)
library(tidyverse)
library(tidytext)
library(stringr)
library(ggplot2)
library(igraph)
library(ggraph)
library(widyr)
library(topicmodels)
library(scales)

my_query <- '0091-6765[TA] AND 2006:2016[DP]'
out <- easyPubMed::get_pubmed_ids(my_query)
data <- easyPubMed::fetch_pubmed_data(out,retmax = 5000)

text_df <- xml2df(data)

data("stop_words")

library(tidyverse)
library(lubridate)
library(tidytext)
library(stringr)

wordf <- text_df %>%
        mutate(time = as.POSIXct(date, origin = "1970-01-01"),
        month = round_date(time, "month")) %>%
        filter(!is.na(abstract)) %>%
        unnest_tokens(word, title) %>%
        anti_join(stop_words) %>%
        filter(str_detect(word, "[^\\d]")) %>%
        group_by(word) %>%
        mutate(word_total = n()) %>%
        ungroup()       
                

words_per_month <- wordf %>%
  group_by(month) %>%
  summarize(month_total = n())

word_month_counts <- wordf %>%
  filter(word_total >= 100) %>%
  count(word, month) %>%
  complete(word, month, fill = list(n = 0)) %>%
  inner_join(words_per_month, by = "month") %>%
  mutate(percent = n / month_total) %>%
  mutate(year = (year(month) + yday(month) / 365))

library(broom)

mod <- ~ glm(cbind(n, month_total - n) ~ year, ., family = "binomial")

slopes <- word_month_counts %>%
  nest(-word) %>%
  mutate(model = map(data, mod)) %>%
  unnest(map(model, tidy)) %>%
  filter(term == "year") %>%
  arrange(desc(estimate))

slopes

slopes %>%
  head(30) %>%
  inner_join(word_month_counts, by = "word") %>%
  mutate(word = reorder(word, -estimate)) %>%
  ggplot(aes(year, n / month_total, color = word)) +
  geom_line(show.legend = FALSE) +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~ word, scales = "free_y") +
  expand_limits(y = 0) +
  labs(x = "Year",
       y = "Percentage of titles containing this term",
       title = "30 fastest growing words in EHP",
       subtitle = "Judged by growth rate over 10 years")

word_counts <- wordf %>%
  count(word, sort = TRUE)
word_counts %>%
  head(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "lightblue") +
  scale_y_continuous(labels = comma_format()) +
  coord_flip() +
  labs(title = "Most common words in EHP",
       subtitle = "Among 2006-2016; stop words removed",
       y = "# of uses")

desc_dtm <- word_counts %>%
        filter(year == '2006') %>%
        cast_dtm(line, word, n)

desc_lda <- LDA(desc_dtm, k = 10, control = list(seed = 42))

tidy_lda <- tidy(desc_lda)
tidy_lda

top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 5, scales = "free")

lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma

ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 5) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))

contributions <- wordf %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions %>%
  top_n(25, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +  geom_col(show.legend = FALSE) +
  facet_wrap(~ year, ncol = 4)
  coord_flip()
```

```{r metagear}

```


```{r scholar}
library(XML)
get_abstract = function(pub_id, my_id) {
  print(pub_id)
  paper_url = "https://scholar.google.ca/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=&as_publication=environmental+science+%26+technology&as_ylo=2017&as_yhi=&btnG=&hl=en&as_sdt=0%2C5"
  paper_page = htmlTreeParse(paper_url, useInternalNodes=TRUE, encoding="utf-8")
  paper_abstract = xpathSApply(paper_page, "//div[@id='gsc_descr']", xmlValue)
  return(paper_abstract)
}

get_all_publications = function(authorid) {
  # initializing the publication list
  all_publications = NULL
  # initializing a counter for the citations
  cstart = 0
  # initializing a boolean that check if the loop should continue
  notstop = TRUE
 
  while (notstop) {
    new_publications = try(get_publications(my_id, cstart=cstart), silent=TRUE)
    if (class(new_publications)=="try-error") {
      notstop = FALSE
    } else {
      # append publication list
      all_publications = rbind(all_publications, new_publications)
      cstart=cstart+20
    }
  }
  return(all_publicationss)
}
```

